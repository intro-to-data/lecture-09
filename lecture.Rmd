---
title: "Linear Regression and Centrality"
output: html_notebook
---

## Setup

We are going to discuss a couple of different data sets and a couple of different ideas tonight.

```{r setup, include=FALSE}
rm(list = ls())
library(modelr)
library(tidyverse)
options(scipen=999)
data(diamonds)
```

# Measures of Centrality

## Height of Adult Women

- The mean (average) and median are the two most common methods for summarizing the central tendency of a data set.
    - This is not a problem, simply an observation of reality.
- The example below is from an R data set of heights of women.
    - This data set contains merely 15 rows of data and should not be treated as canonical.
- That said, it is a great example of an average tells us something meaningful about the data.
    - The black line is the density plot. Remember, the area under a density plot = 1.
    - The average height is marked in red.
    - The blue lines are for the average +/- twice the standard deviation. These are our 95th percentile markers.
    - This looks good. Really. It does.

```{r}
data(women)
avg_height <- mean(women$height/12)
sd_height <- sd(women$height/12)

ggplot(women, aes(height/12)) +
  geom_density() +
  geom_vline(aes(xintercept = avg_height), color = "red") +
  geom_vline(aes(xintercept = avg_height + 2 * sd_height), color = "blue") +
  geom_vline(aes(xintercept =  avg_height - 2 * sd_height), color = "blue")
```

## Insurance Claims Cost

- The code I wrote to perform this analsis is in this repo, but the raw data is not.
    - claims2018.sql The SQL I wrote to export the data from our EDW.
    - claims2018.R: Imports the data from CSV and calculates the average, etc.
- I cannot share this data with you because it (obviously) contains PHI.
    - This data is from 2018, avoiding all impact from Covid, etc.
    - Members are people living here in the Capital District and surrounding counties.
        - I am not at liberty to be any more precise than that.
    - With a few exceptions, all claims are included.
        - This data set excludes certain claim types which are extra sensitive including some HIV treatments, certain mental health diagnoses, etc.
    - And what do we find? **CRAZINESS!**

```{r}
knitr::include_graphics("images/clams-2018-zoomed-out.png")
```

- Colors have the same meaning as before however, our average is not even remotely close to the center of our plot.
    - Some people are really, really expensive.
- Anytime your 95% confidence interval includes impossible numbers, you probably have a problem.
    - There are women shorter than 5 feet tall.
    - Not counting some funkiness around deductions spanning calendar years and what not, people do not have less than $0 healthcare costs.
        - Says the guy who hasn't seen his PCP in . . . ? years . . . do I even have a PCP? Hmm. I should know the answer to that question.

Let's "zoom in" on just those patients with less than $50,000 in 2018 costs and include the median, because the median is more resistant to outliers, right?

- Well, maybe? Our upper limit on the 95% confidence interval is ~$34,500.
- Would any of you want to argue that the red line is representative of the center of the data?
- And even the median isn't actually all that great (although it _is_ much better).

```{r}
knitr::include_graphics("images/clams-2018-zoomed-in.png")
```

- This average is mathematically, the average. It isn't "wrong" but I would argue it isn't very meaningful.
- Moreover, the median, while better, is also a little weird.
- And no single summary number, no matter how brilliantly conceived can express the long-tail of this distribution.
    - A picture really is worth a thousand measures of central tendency.
    - Because while true, I still think it is misleading to say the median cost was 784 dollars, simply because so many people spent SO MUCH LESS.
         - In truth, MOST PEOPLE spent less.


# Linear Regression

**Question:** Can we predict the price of a diamond?
**Answer:** Yes, but it is a little harder than you might expect.

And just in case you haven't been reading the book, [tonight's lecture looks very similar to a chapter in our book](https://r4ds.had.co.nz/model-building.html).

Given the data below, we will develop a model to predict the price of a diamond.

```{r}
diamonds
```

- At first, this seems like it should be easy, but things are not quite as simple as they appear to be.
- In the graphs below, be aware: 
    - The worst diamond color is J (slightly yellow). 
    - The worst clarity is I1 (inclusions visible to the naked eye).

```{r}
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
ggplot(diamonds, aes(color, price)) + geom_boxplot()
ggplot(diamonds, aes(clarity, price)) + geom_boxplot()
```

This is . . . . odd? It would seem that crappy diamonds cost more than good diamonds . . . . because of course they do. Maybe.

```{r}
## This draws a hex-scatter plot and rather than count each individual diamond
## separately, it creates a number of bins (N) across the x and y axis and then
## reports how many cases (diamonds) are in each bin.
ggplot(diamonds, aes(carat, price)) + 
  geom_hex(bins = 50)
```

- Ahhhhhhhhh. Carat.
- The WEIGHT/SIZE of the thing matters more than anything else. 
- And, side-note, that ain't a linear relationship.
- Which makes sense, because you can see that beyond two carats, there are fewer and fewer diamonds for sale at that weight.
- And if ~2 is a big diamond, it stands to reason that it might start to have quality problems.

**Question:** So, how do we handle non-linear relationships?
**Answer:** We log the hell out of them!

- The linear model assumes a linear relationship between the dependent (price) and independent variable (carat).
- And we can see this is not even remotely true.
- To deal with this, we can do two things:

- Focus on diamonds smaller than 2.5 carats (99.7% of the data)
- Log-transform the carat and price variables.

```{r}
## Note: I am creating a NEW copy of diamonds.
## Note 2: I am using mutate to create new columns in the data.
diamonds2 <- 
  diamonds %>%
  filter(carat <= 2.5) %>%
  mutate(lprice = log2(price),
         lcarat = log2(carat))
```

- We don't have time to fully unpack what a logarithm is and how it normalizes our data, so if you would like to take a deep dive into that topic: https://www.youtube.com/watch?v=cEvgcoyZvB4
- But I do want to prove to you that it works.

```{r}
ggplot(diamonds2, aes(price)) + geom_density()
ggplot(diamonds2, aes(lprice)) + geom_density()
```

- Please note that the SHAPE of the resulting data would be the same regardless of which log we used. 
- Common options here include natural log (`log()`), base-2 log (`log2()`), and base-10 log (`log10()`).
- Feel free to play with this code to prove this to yourself.
- Whew, while it is still multi-modal (because the underlying data is), it does remove our skew.

```{r}
## Same plot as before, this time using logged data.
ggplot(diamonds2, aes(lcarat, lprice)) + 
  geom_hex(bins = 50)
```

Assumptions of Linear Regression:

- Weak exogeneity: Independent variables are assumed to be error free.
    - ROTFLMAO!
    - I mean, sure, we can measure weight well.
    - Other things? Not so much.
- Linearity: The relationship between our independent and dependent variable needs to be linear (or nearly so).
    - OK, we fixed that.
- Constant variance: Our dependent variable is not assumed to be error free, that we _do_ assume that error is random.
- Independence of errors. This assumes that the errors of the response variables are uncorrelated with each other.
- Lack of perfect multicollinearity in the predictors: Your independent variables cannot be correlated.
    - For example, if color and clarity are correlated, that is bad.
    
```{r}
## Same plot as before, this time using logged data.
diamonds2 %>%
  group_by(color, clarity) %>%
  count() %>%
  ggplot(aes(color, clarity, fill = n)) + 
  geom_tile()
```

- We can reasonably claim these are not perfectly correlated.

```{r}
mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)
summary(mod_diamond)
```

- Let's briefly discuss what it is we have done.
- We trained this data on the full sample, which we will discuss as less than ideal in the coming weeks.
- But, what have we really done?

```{r}
demo <- sample_n(diamonds2, 100)
demo <- add_predictions(demo, mod_diamond)
demo %>% 
  arrange(pred) %>%
  ggplot() +
  geom_point(aes(x = lcarat, y = lprice)) +
  geom_line(aes(x = lcarat, y = pred), color = "blue")
```

- The vertical difference (positive and negative) between the blue line (our prediction) AND the black dots (real diamond price) are called the residual.
- And we want these residuals to be normallmod_diamond <- lm(lprice ~ lcarat, data = diamonds2)
summary(mod_diamond)y distributed.
- The goal of any model is to reduce this size of the residuals.

```{r}
demo <- add_residuals(demo, mod_diamond)
demo %>%
  ggplot(aes(resid)) + geom_density()
```

And we can see that our residuals are not perfectly distributed, but not terribly so. Perhaps we can better understand what is going on here.

```{r}
ggplot(demo, aes(cut, resid)) + geom_boxplot()
ggplot(demo, aes(color, resid)) + geom_boxplot()
ggplot(demo, aes(clarity, resid)) + geom_boxplot()
```

- Cut, color, and clarity have a relationship with our residuals.
- And the strongest appears to be clarity.

```{r}
mod_diamond2 <- lm(lprice ~ lcarat + clarity, data = diamonds2)
summary(mod_diamond2)
```
- Using clarity, we went from an R-squared of .9334 to .9655.
- Remember, R^2 is the percent of the variation in lprice explained by the model.
- And we can prove that clarity was the next most important variable.

```{r}
demo <- add_residuals(demo, mod_diamond2)
demo <- add_predictions(demo, mod_diamond2)
demo %>%
  ggplot(aes(resid)) + geom_density()
```

And, you can see that we have:

- Reduced the min/max of our residuals.
- Our residuals are more normally distributed.
- And we can look at our actual results.

```{r}
demo %>%
  mutate(true_predicted_price = 2^pred) %>%
  select(price, true_predicted_price)
```

- And, we can see that this is a pretty legitimate model.
- In an upcoming lab, you will get to explore different ways to make it better.

# More reading:
 - https://www.statisticssolutions.com/assumptions-of-linear-regression/
 - https://en.wikipedia.org/wiki/Linear_regression
 
 
